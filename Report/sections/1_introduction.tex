\section{Introduction}\label{sec:introduction}

In the field of information retrieval, finding relevant information efficiently and effectively from large collections has been a challenge for a long time. This task involves finding documents or passages that are most relevant to a particular query or topic. Conventional methods of information retrieval often rely on sparse representations such as BM-25 (Robertson and Zaragoza \cite{BM25}), which can miss subtle semantic connections and fail to fully capture the context of the information being searched for.

To address these limitations, dense retrieval has emerged as an alternative approach, utilizing dense vector embeddings of documents and queries. These embeddings, often generated using large language models such as BERT (Devlin et al. \cite{bert}), encode richer semantic information, enabling a more comprehensive understanding of content. Two main approaches to dense retrieval are the bi-encoder and cross-encoder methods. The bi-encoder approach creates separate vector representations for queries and documents, while the cross-encoder approach directly measures the similarity between the query and document, potentially yielding more accurate results at a higher computational cost.

Recently, information retrieval has evolved from solely ranking matching documents to a 'rich search' or 'Entity-oriented search' (Balog \cite{balog2018entity}). 'Rich search' implies that queries often require specific information about entities, facts, or structured data for more sophisticated tasks.

However, large language models like BERT suffer from an issue within this context: entities such as people, places, or organizations are not entirely represented in the models (Heinzerling and Inui \cite{limitations}). Thus, large language models are likely to represent socially relevant entities that occur in frequent documents, but at the same time entities that are rather rare not.

As an example, the two authors refer to the training process of BERT model, which the well-known masked language model approach. During training phase, the model is charged to predict randomly masked tokens based on the context of the surrounding words. In the example, 'Paris is the capital of [MASK]' (Heinzerling and Inui \cite{limitations}), 'France' is correctly predicted with a high probability, demonstrating the model's ability to recognize frequent entities. However, prediction might be ambiguous for rarer entities like 'Sesame Street' in the case of 'Bert is a character on [MASK]' (Heinzerling and Inui \cite{limitations}). In particular, entities that were not present during the training process and only emerged afterwards cannot be captured within language models at all.

This limitation motivated Tran and Yates \cite{tran2022dense} to explore solutions in their paper, which is the subject of this seminar report. The subsequent sections will present and critically examine Tran and Yates' work. We will contextualize the paper in current research in \autoref{sec:related_work}, explain the methods used in \autoref{sec:methods}, present the results in \autoref{sec:results}, and conclude with a critical assessment and personal opinion in \autoref{sec:discussion}.

