\section{Introduction}\label{sec:introduction}

In the field of information retrieval, finding relevant information efficiently and effectively from large collections has been a challenge for a long time. This task involves finding documents or passages that are most relevant to a particular query or topic. Conventional methods of information retrieval such as BM-25 (Robertson and Zaragoza \cite{BM25}) often rely on sparse representations of documents, that miss subtle semantic connections and fail to fully capture the context of the information being searched.

To address the limitations of sparse retrieval approach, dense retrieval has emerged as an alternative approach. This approach entails the utilization of dense vector representations for documents and queries, signifying continuous-valued vectors situated within a high-dimensional space, commonly referred to as embeddings. In the context of dense retrieval these are mostly built using pre-trained language models such as BERT (Devlin et al. \cite{bert}) to encode rich semantic information and enabling comprehensive understanding of content. 

Two main approaches to dense retrieval are the bi-encoder and cross-encoder methods. The bi-encoder approach involves creating distinct embeddings for queries and documents, which are then compared by similarity measurements as cosine similarity. On the other hand, the cross-encoder approach directly measures the similarity between the query and document by taking both as input for its pre-trained language model. This approach results in increased computational expenses as a consequence of performing model inference for each pair of query and document.

Recently, information retrieval has evolved from solely ranking matching documents to a 'rich search' or 'Entity-oriented search' (Balog \cite{balog2018entity}). 'Rich search' implies that queries often require specific information about entities, facts, or structured data for more sophisticated tasks.

However, pre-trained language models like BERT suffer from an issue within this context: entities such as people, places, or organizations are not entirely represented in the models (Heinzerling and Inui \cite{limitations}). To substantiate this assertion, Heinzerling and Inui refer to the training process of BERT model, which utilizes the well-known masked language model approach. During training phase, the model is demanded to predict randomly masked tokens based on the context of the surrounding words. In the example, 'Paris is the capital of [MASK]' (Heinzerling and Inui \cite{limitations}), 'France' is correctly predicted with a high probability, demonstrating the model's ability to recognize frequent entities. However, prediction might be ambiguous for rarer entities like 'Sesame Street' in the case of 'Bert is a character on [MASK]' (Heinzerling and Inui \cite{limitations}). In particular, entities that were not present during the training process and only emerged afterwards cannot be captured within pre-trained language models.

This limitation motivated Tran and Yates \cite{tran2022dense} to explore solutions in their work 'Dense Retrieval with Entity Views', which is the subject of this seminar report. The subsequent sections will present and critically examine Tran and Yates' work. First, I will contextualize the paper in current research in \autoref{sec:related_work}, explain the methods used in \autoref{sec:methods}, present the results in \autoref{sec:results}, and conclude with a critical assessment and personal opinion in \autoref{sec:discussion}.

