\section{Introduction}\label{sec:introduction}

In the field of information retrieval, finding relevant information efficiently and effectively from large collections has been a challenge for a long time. This task involves finding documents or passages that are most relevant to a particular query or topic. Conventional methods of information retrieval often rely on sparse representations such as BM-25 (Robertson and Zaragoza \cite{BM25}), which can miss subtle semantic connections and fail to fully capture the context of the information being searched for.

Dense retrieval, on the other hand, has emerged as another approach that attempts to overcome the limitations of sparse representations by using dense vector embeddings of documents and queries. These embeddings are usually generated using large language models such as BERT (Devlin et. al \cite{bert}) and allow to encode richer semantic information. Consequently, they provide a more comprehensive understanding of the underlying content. 

For dense retrieval, two fundamentally different approaches have evolved: bi-encoder and cross-encoder. The bi-encoder approach involves creating distinct vector representations for queries and documents, which are then compared to determine relevance scores, while the cross-encoder approach directly measures the similarity between the query and document by taking both as input, potentially leading to more accurate results but with higher computational cost.

Apart from that, the field of information retrieval seems to evolve from a more classical approach of only ranking best matching documents of a related query to a 'rich search' (Balog \cite{balog2018entity}) ranking, which is often referred to 'Entity-oriented search', as Balog \cite{balog2018entity} describes. In this context, 'rich search' refers to the fact that answers to queries more often contain specific information about entities, facts or other structured data. In consequence, information about entities are quite relevant to solve this more sophisticated task.

However, large language models such as BERT tend to suffer from a problem that arises particularly in the context of querying documents: entities such as persons, locations, or organizations are not represented entirely within the models, as Heinzerling and Inui \cite{limitations} were able to illustrate. Thus, large language models are likely to represent socially relevant entities that occur in frequent documents, but at the same time entities that are rather rare not.

As an example, the two authors refer to the training process of BERT model, which uses a masked language model approach. During training phase, specific words or tokens are taken from existing documents and are randomly masked. The model is then charged to predict the masked tokens based on the context of the surrounding words. France is therefore correctly predicted with a high probability for the example 'Paris is the capital of [MASK]' (Heinzerling and Inui, \cite{limitations}), while prediction might be ambiguous for the rather rare entity Sesame Street in the case of 'Bert is a character on [MASK]' (Heinzerling and Inui, \cite{limitations}). In particular, entities that were not present during the training process and only emerged afterwards cannot be captured within language models at all.

The outlined relevance of entities in the retrieval of relevant information and the issue that these entities are not or only partially captured by large language models were the major reasons for Tran and Yates \cite{tran2022dense} to work on the paper that is presented in this seminar report. In the following, the work of Tran and Yates will be presented in detail and critically examined. The elaborations will be structured as follows: First, in \autoref{sec:related_work} the work is put into the context of current research in the field of information retrieval. Then, in \autoref{sec:methods}, the ideas and methods used by Tran and Yates are presented. This is followed by a presentation of the results in \autoref{sec:results}, before concluding with a critical assessment and personal opinion in \autoref{sec:discussion}.

