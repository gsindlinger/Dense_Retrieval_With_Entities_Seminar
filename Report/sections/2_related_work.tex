\section{Related Work}\label{sec:related_work}

Entities have been a subject of interest in the field of information retrieval since the emergence of knowledge repositories like Wikipedia. Research in this context can be divided into the time before and after the advent of large language models such as BERT \cite{bert}.

\subsection{Entities in Sparse Retrieval}\label{subsec:sparse}

Prior to the advent of large language models such as BERT \cite{bert}, research focused on sparse retrieval methods like BM25, where entities from knowledge bases were used to enhance query understanding.  Efforts were made to expand queries using Wikipedia descriptions of entities \cite{xu2009query} and extract additional features like synonyms and relationships from linked knowledge bases \cite{dalton2014entity}. Balog \cite{balog2018entity} provided a comprehensive meta-analysis of related research.

\subsection{Entities in Dense Retrieval}\label{subsec:dense}

While some research has already addressed the consideration of entities in sparse retrieval, the amount of work in the field of dense retrieval in this context is rather limited. 

Some work focuses on integration of entities within interaction-based methods, treating textual and entity information as common inputs for ranking models. An example of this approach is the work of Xiong et al. \cite{xiong2017word,xiong2017word_2}: They developed a method to build ranking features by incorporating an attention mechanism of word embeddings and entity embeddings and could significantly outperform baselines for word-based and entity-based learning to rank systems.

Interaction-based methods such as these can be considered as extensions of the cross-encoder approach of dense retrieval and therefore potentially face the same issue of high computational complexity as described in \autoref{sec:introduction}. All documents and query pairs must be processed at runtime, which can lead to significant time and resource overhead. This leads to a slow overall retrieval process and can be inefficient, especially with large data sets.

In the context of the broad field of natural language processing tasks, entities have also been considered in several research papers and have been shown to have a significant impact on its tasks (e.g. ). The most prominent example is the ERNIE model (Sun et al. \cite{ernie}), which incorporates knowledge from both pre-training tasks and external knowledge graphs, enabling it to achieve better contextual understanding and knowledge integration in natural language processing tasks. 

\begin{comment}
    ERNIE builds upon BERT's methods, particularly the masked language model, and tailors them to a more context-sensitive learning approach. ERNIE employs a masking strategy during its learning procedure, where the model is required to predict not only single words or tokens, but also several consecutive words. These consecutive words originate from different subtasks within the ERNIE model, including basic-level masking, phrase-level masking, and entity-level masking. The basic-level masking follows the standard token masking approach used in BERT, while the phrase-level masking groups small sets of tokens together to form conceptual units in the language to learn. Of most relevance to this seminar report is the entity-level masking stage, where the model tries to accurately predict entities that can span multiple tokens. This leads to the model becoming highly sensitive towards entities, as demonstrated by Sun et al. in their work on ERNIE \cite{ernie}. ERNIE can be fine-tuned for information retrieval tasks, offering valuable insights and serving as a reference model for Tran and Yates' work \cite{tran2022dense}.
\end{comment}

The existing concepts of dense retrieval, including interaction-based methods and models like ERNIE, do not fully explore entities independently from the underlying large language model for the traditional search retrieval task. This represents the main novelty of Tran and Yates' work \cite{tran2022dense}, which will be elaborated in the following sections.



