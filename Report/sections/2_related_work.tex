\section{Related Work}\label{sec:related_work}

Entities have been a subject of interest in the field of information retrieval since the emergence of knowledge repositories like Wikipedia. Research in this context can be divided into the time before and after the advent of pre-trained language models such as BERT.

\subsection{Entities in Sparse Retrieval}\label{subsec:sparse}

Prior to the advent of pre-trained language models such as BERT, research focused on sparse retrieval methods like BM-25, where entities from knowledge bases were used to enhance query understanding. Recent work in this context focuses mainly on extending queries with additional data: Xu et al. \cite{xu2009query} proposed a method to expand queries by Wikipedia descriptions of entities. Other researchers like Dalton et al. \cite{dalton2014entity} extended this idea and extracted more sophisticated features out of the linked knowledge bases like synonyms or relationships to other entities. The research in this field is quite extensive and has been summarized in detail in a meta-work by Balog \cite{balog2018entity}.

\subsection{Entities in Dense Retrieval}\label{subsec:dense}

While some research has already addressed the consideration of entities in sparse retrieval, the amount of work in the field of dense retrieval in this context is rather limited. 

Some work focuses on integration of entities within interaction-based methods: An example of this approach is the work of Xiong et al. \cite{xiong2017word,xiong2017word_2}. They developed a method to build ranking features by incorporating an attention mechanism of word embeddings and entity embeddings and significantly outperformed baselines for word-based and entity-based learning to rank systems. Interaction-based methods can be considered as extensions of the cross-encoder approach of dense retrieval and therefore face the issue of high computational complexity as described in \autoref{sec:introduction}. All documents and query pairs must be inferred, which can lead to significant time and resource overhead. This leads to a slow overall retrieval process and can be inefficient, especially with large data sets.

In the context of the broad field of natural language processing tasks, entities have been considered in several research papers and shown to have a significant impact on its tasks (e.g. Liu et al. \cite{liu2020k}, Peters et al. \cite{peters2019knowledge}). ERNIE model proposed by Sun et al. \cite{ernie} incorporates knowledge from both pre-training tasks and external knowledge graphs, enabling it to achieve better contextual understanding and knowledge integration in natural language processing tasks. 

\begin{comment}
    ERNIE builds upon BERT's methods, particularly the masked language model, and tailors them to a more context-sensitive learning approach. ERNIE employs a masking strategy during its learning procedure, where the model is required to predict not only single words or tokens, but also several consecutive words. These consecutive words originate from different subtasks within the ERNIE model, including basic-level masking, phrase-level masking, and entity-level masking. The basic-level masking follows the standard token masking approach used in BERT, while the phrase-level masking groups small sets of tokens together to form conceptual units in the language to learn. Of most relevance to this seminar report is the entity-level masking stage, where the model tries to accurately predict entities that can span multiple tokens. This leads to the model becoming highly sensitive towards entities, as demonstrated by Sun et al. in their work on ERNIE \cite{ernie}. ERNIE can be fine-tuned for information retrieval tasks, offering valuable insights and serving as a reference model for Tran and Yates' work \cite{tran2022dense}.
\end{comment}

The existing concepts of dense retrieval, including interaction-based methods and models like ERNIE, do not fully explore entities independently from the underlying pre-trained language model for the traditional retrieval task. This represents the main novelty of Tran and Yates' work \cite{tran2022dense}, which will be elaborated in the following sections.



