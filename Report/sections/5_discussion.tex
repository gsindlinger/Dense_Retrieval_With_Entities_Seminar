\section{Discussion \& Criticism}\label{sec:discussion}

Tran and Yates propose an approach that demonstrates the significant improvement of classical dense retrieval methods through the incorporation of entity information. The results, surpassing the baselines TAS BERT and ERNIE, support the effectiveness of their method. Several positive aspects and some limitations of their work can be identified.

\subsection{Positives}

The approach by Tran and Yates demonstrates superior performance over the baseline methods, as elucidated in the outcomes section. This substantiates the rationale behind the introduction of their approach.

The concepts and elucidations provided by Tran and Yates are marked by simplicity and coherence. The introduced methods (see \autoref{sec:methods}) have a clear structure. For instance, the choice of aggregation method for embeddings of word tokens and entities, i.e. concatenation of embeddings (see \autoref{fig:general_model}), is easier to understand than alternative approaches like max pooling or sum pooling. Based on that, the system could be extended with additional embeddings without much effort.

While many research efforts focus solely on achieving top results on leader boards, Tran and Yates' approach appears to be more oriented towards practical use and sustainability. The EVA Multi-approach offers both high effectiveness and efficiency.

\subsection{Negatives}

Tran and Yates concentrate solely on the impact of entities in their work, but their analyses reveal that for many queries, entities play no significant role. As shown in \autoref{tab:query_statistics}, 43.5 \% of all queries in the training data do not contain entities, causing the EVA models being ineffective for such queries.

A further drawback of the approach is its lack of originality. Tran and Yates' propositions are rooted in pre-existing frameworks, with their primary contribution stemming from the combination of ideas from prior authors. As a result, the approach, while intuitive, cannot be classified as groundbreaking. This is evident from the limited citations of their work so far, with only one citation in Kamphuis et al. \cite{kamphuis2023mmead}.

\subsection{Possible Extensions}
The granular structure of Tran and Yates' models allows for possible extensions through the exchange or addition of individual components. For instance, the pre-trained language model used to calculate word-level embeddings could be extended beyond the two baseline models TAS BERT and ERNIE to other, more sophisticated models. In domain-specific use cases, a tailored choice of the pre-trained model might further enhance effectiveness. For example, in a biomedical context, BioBERT (Lee et. al \cite{lee2020biobert}) could be considered.

The embedding component for entities could also be replaced or supplemented with other choices. For example, keyword embeddings, as presented by Gab{'\i}n et al. \cite{gabin2023keyword}, or structural information, such as that presented by Raman et al. \cite{raman2022structure}, could serve as alternative sources for external embeddings. Furthermore, in the context of HTML files, Guo et al.'s approach \cite{guo2022webformer} could be explored for generating embeddings.

These extensions could address some limitations of the current approach and potentially lead to further improvements in effectiveness and applicability in different domains.




