\section{Discussion \& Criticism}\label{sec:discussion}

Tran and Yates propose an approach that demonstrates the significant improvement of classical dense retrieval methods through the incorporation of entity information. The results, surpassing the baselines TAS BERT and ERNIE, support the effectiveness of their method. Several positive aspects and some limitations of their work can be identified.

\subsection{Positives}

\begin{itemize}
\item Satisfying results: As demonstrated in \autoref{subsec:outcomes}, Tran and Yates' approach outperforms the baselines, justifying the introduction of their method.
\item Simple and intuitive approach: The ideas and explanations provided by Tran and Yates are straightforward to follow. The introduced methods, from the general model (see \autoref{subsec:general_model}) to the algorithms used (e.g., algorithm \ref{alg:query-aware-entity-representation}), have a clear structure. For instance, the choice of aggregation method for embeddings of word tokens and entities, i.e. concatenation of embeddings (see \autoref{fig:general_model}), is easier to understand than alternative approaches like max pooling or sum pooling. Based on that, the system could be extended with additional embeddings without much effort.
\item Consideration of both effectiveness and efficiency: While many research efforts focus solely on achieving top results on leader boards, Tran and Yates' approach appears to be more oriented towards practical use and sustainability. The EVA Multi-approach offers both high effectiveness and efficiency.
\end{itemize}

\subsection{Negatives}

\begin{itemize}
\item Entities aren't universally relevant: Tran and Yates concentrate solely on the impact of entities in their work, but their analyses reveal that for many queries, entities play no significant role. As shown in \autoref{tab:query_statistics}, 43.5 \% of all queries in the training data do not contain entities, causing the EVA models being ineffective for such queries.
\item Lack of originality: The ideas presented by Tran and Yates are based on existing frameworks, and their contribution mainly lies in the composition of ideas from other authors. As a result, the approach, while intuitive, cannot be classified as groundbreaking. This is evident from the limited citations of their work so far, with only one citation in Kamphuis et al. \cite{kamphuis2023mmead}.
\end{itemize}

\subsection{Possible Extensions}
The granular structure of Tran and Yates' models allows for possible extensions through the exchange or addition of individual components. For instance, the pre-trained language model used to calculate word-level embeddings could be extended beyond the two baseline models TAS BERT and ERNIE to other, more sophisticated models. In domain-specific use cases, a tailored choice of the pre-trained model might further enhance effectiveness. For example, in a biomedical context, BioBERT (Lee et. al \cite{lee2020biobert}) could be considered.

The embedding component for entities could also be replaced or supplemented with other choices. For example, keyword embeddings, as presented by Gab{'\i}n et al. \cite{gabin2023keyword}, or structural information, such as that presented by Raman et al. \cite{raman2022structure}, could serve as alternative sources for external embeddings. Furthermore, in the context of HTML files, Guo et al.'s approach \cite{guo2022webformer} could be explored for generating embeddings.

These extensions could address some limitations of the current approach and potentially lead to further improvements in effectiveness and applicability in different domains.




